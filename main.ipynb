{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from logger import setup_logger\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up API keys and environment variables\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "LANGCHAIN_API_KEY = os.getenv('LANGCHAIN_API_KEY')\n",
    "WORKING_DIRECTORY = os.getenv('WORKING_DIRECTORY', './data_storage/')\n",
    "\n",
    "# Validate critical environment variables\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY is not set in the environment variables.\")\n",
    "if not LANGCHAIN_API_KEY:\n",
    "    raise ValueError(\"LANGCHAIN_API_KEY is not set in the environment variables.\")\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Multi-Agent Data Analysis System\"\n",
    "\n",
    "# Set up logger\n",
    "logger = setup_logger()\n",
    "\n",
    "# Initialize language models\n",
    "try:\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, max_tokens=4096)\n",
    "    process_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, max_tokens=4096)\n",
    "    json_llm = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        model_kwargs={\"response_format\": {\"type\": \"json_object\"}},\n",
    "        temperature=0,\n",
    "        max_tokens=4096\n",
    "    )\n",
    "    logger.info(\"Language models initialized successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error initializing language models: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Ensure working directory exists\n",
    "if not os.path.exists(WORKING_DIRECTORY):\n",
    "    os.makedirs(WORKING_DIRECTORY)\n",
    "    logger.info(f\"Created working directory: {WORKING_DIRECTORY}\")\n",
    "\n",
    "logger.info(\"Initialization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from state import State\n",
    "from node import agent_node,human_choice_node,note_agent_node\n",
    "from create_agent import create_agent,create_supervisor\n",
    "from router import QualityReview_router,hypothesis_router,process_router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create state graph for the workflow\n",
    "workflow = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "members = [\"Hypothesis\",\"Process\",\"Visualization\", \"Search\", \"Coder\", \"Report\", \"QualityReview\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.internet import google_search,FireCrawl_scrape_webpages\n",
    "from tools.basetool import execute_code,execute_command\n",
    "from tools.FileEdit import create_document,read_document,edit_document,collect_data\n",
    "from langchain.agents import load_tools\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "hypothesis_agent = create_agent(\n",
    "llm, \n",
    "[collect_data,wikipedia,google_search,FireCrawl_scrape_webpages]+load_tools([\"arxiv\"],),\n",
    "'''\n",
    "As an esteemed expert in data analysis, \n",
    "your task is to formulate a set of research hypotheses and outline the steps to be taken based on the information table provided. \n",
    "Utilize statistics, machine learning, deep learning, and artificial intelligence in developing these hypotheses. \n",
    "Your hypotheses should be precise, achievable, professional, and innovative. To ensure the feasibility and uniqueness of your hypotheses, \n",
    "thoroughly investigate relevant information. For each hypothesis, include ample references to support your claims.\n",
    "\n",
    "Upon analyzing the information table, you are required to:\n",
    "\n",
    "1.Formulate research hypotheses that leverage statistics, machine learning, deep learning, and AI techniques.\n",
    "2.Outline the steps involved in testing these hypotheses.\n",
    "3.Verify the feasibility and uniqueness of each hypothesis through a comprehensive literature review.\n",
    "At the conclusion of your analysis, present the complete research hypotheses, elaborate on their uniqueness and feasibility, \n",
    "and provide relevant references to support your assertions. Please answer in structured way to enhance readability.\n",
    "Just answer a research hypothesis.\n",
    "''',\n",
    "members,WORKING_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_agent = create_supervisor(\n",
    "    process_llm,\n",
    "    '''\n",
    "    As research supervisor, coordinate a comprehensive data analysis process resulting in a complete report. Manage \"Visualization\", \"Search\", \"Coder\", and \"Report\" agents to achieve the following:\n",
    "\n",
    "    1. Validate and refine the research hypothesis.\n",
    "    2. Conduct thorough data analysis with well-documented code.\n",
    "    3. Produce a complete research report including:\n",
    "       - Introduction\n",
    "       - Hypothesis\n",
    "       - Methodology\n",
    "       - Results with relevant visualizations\n",
    "       - Discussion\n",
    "       - Conclusion\n",
    "       - References\n",
    "\n",
    "    For each step:\n",
    "    1. Define clear objectives and outcomes.\n",
    "    2. Assign specific tasks to appropriate agent(s).\n",
    "    3. Review and integrate agent outputs.\n",
    "    4. Ensure quality and relevance of all contributions.\n",
    "\n",
    "    Agent Guidelines:\n",
    "    - Visualization: Create and explain insightful data visualizations.\n",
    "    - Search: Gather relevant information and compile references.\n",
    "    - Coder: Develop efficient, documented data analysis code.\n",
    "    - Report: Draft and refine report sections.\n",
    "\n",
    "    Workflow:\n",
    "    1. Plan the analysis process.\n",
    "    2. Assign and oversee agent tasks.\n",
    "    3. Integrate insights across agents.\n",
    "    4. Refine the analysis based on emerging results.\n",
    "    5. Compile the final report.\n",
    "\n",
    "    After each agent task, evaluate output and determine the next step. Respond with the next agent to act or \"FINISH\" only when:\n",
    "    1. Hypothesis is thoroughly tested.\n",
    "    2. Analysis is complete with documented code.\n",
    "    3. All visualizations are created and explained.\n",
    "    4. Comprehensive report is written with all sections.\n",
    "    5. Reference list is complete.\n",
    "    6. All components are integrated cohesively.\n",
    "\n",
    "    Ensure a logical flow, address all hypothesis aspects, and deliver a cohesive, insightful final report meeting high academic standards.\n",
    "    ''',\n",
    "    [\"Visualization\", \"Search\", \"Coder\", \"Report\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "visualization_agent = create_agent(\n",
    "    llm, \n",
    "    [read_document, execute_code, execute_command],\n",
    "    '''\n",
    "    You are a data visualization expert responsible for creating insightful visualizations. Your tasks include:\n",
    "    1. Analyzing data and creating appropriate visualizations.\n",
    "    2. Interpreting visual patterns and trends in the data.\n",
    "    3. Suggesting improvements for existing visualizations.\n",
    "    4. Explaining the significance of visual findings.\n",
    "\n",
    "    When asked to create a visualization, provide Python code that uses libraries like matplotlib, seaborn, or plotly. Ensure your code is complete and ready to execute. Include comments to explain key parts of your code.\n",
    "    ''',\n",
    "    members,WORKING_DIRECTORY\n",
    "    )\n",
    "\n",
    "code_agent = create_agent(\n",
    "    llm,\n",
    "    [read_document,execute_code, execute_command],\n",
    "    '''You are an expert Python programmer specializing in data analysis and visualization. Your responsibilities include:\n",
    "    1. Writing efficient and clean Python code for data analysis tasks.\n",
    "    2. Implementing statistical methods and machine learning algorithms.\n",
    "    3. Creating informative and visually appealing data visualizations.\n",
    "    4. Debugging and optimizing existing code.\n",
    "    5. Ensuring code readability and adherence to PEP 8 standards.\n",
    "\n",
    "    Important guidelines for visualization and image-related tasks:\n",
    "    1. Always save generated plots or images to files.\n",
    "    2. Use a consistent naming convention for saved files, e.g., 'plot_YYYYMMDD_HHMMSS.png'.\n",
    "    3. Include the file path of the saved image in your code output.\n",
    "    4. Use matplotlib's 'Agg' backend to ensure plots can be saved without a GUI.\n",
    "\n",
    "    When asked to write code, provide only valid Python code as a string. Do not include any explanatory text outside of code comments. Ensure all string literals are properly terminated. The code should be ready to execute without any modifications.\n",
    "\n",
    "    Here's a template for saving plots:\n",
    "\n",
    "    ```python\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')  # Use the 'Agg' backend\n",
    "    import matplotlib.pyplot as plt\n",
    "    from datetime import datetime\n",
    "\n",
    "    # Your plotting code here\n",
    "    # ...\n",
    "\n",
    "    # Save the plot\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f'plot_name_timestamp.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.close()  # Close the plot to free up memory\n",
    "\n",
    "    print(f\"Plot saved as: filename\")\n",
    "    ```\n",
    "\n",
    "    Always include similar code for saving any generated visualizations or images.\n",
    "    ''',\n",
    "    members,WORKING_DIRECTORY\n",
    ")\n",
    "\n",
    "searcher_agent= create_agent(\n",
    "    llm,\n",
    "    [read_document, collect_data,wikipedia,google_search,FireCrawl_scrape_webpages]+load_tools([\"arxiv\"],),\n",
    "    '''You are a skilled research assistant responsible for gathering relevant information. Your tasks include:\n",
    "    1. Conducting comprehensive literature reviews using academic databases.\n",
    "    2. Searching the web for current and relevant information on specified topics.\n",
    "    3. Evaluating the credibility and relevance of sources.\n",
    "    4. Summarizing key findings from your searches.\n",
    "\n",
    "    When providing information, always cite your sources. Prioritize peer-reviewed academic sources, but also include reputable web sources when appropriate. Present your findings in a clear, concise manner.''',\n",
    "    members,WORKING_DIRECTORY\n",
    "    )\n",
    "\n",
    "report_agent = create_agent(\n",
    "    llm, \n",
    "    [create_document, read_document, edit_document], \n",
    "    '''You are an experienced scientific writer responsible for creating comprehensive research reports. Your primary goal is to produce a thorough and well-structured data analysis report. Your tasks include:\n",
    "\n",
    "    1. Formulating and clearly stating the research hypothesis based on the given data and analysis objectives.\n",
    "    2. Detailing the methodology used for data collection and analysis.\n",
    "    3. Synthesizing information from various sources into a coherent report.\n",
    "    4. Structuring the report logically with clear sections:\n",
    "       - Executive Summary\n",
    "       - Introduction (including the research hypothesis)\n",
    "       - Methodology\n",
    "       - Results\n",
    "       - Discussion\n",
    "       - Conclusion\n",
    "       - References\n",
    "    5. Ensuring clarity and precision in scientific writing.\n",
    "    6. Incorporating and explaining relevant data visualizations (charts, graphs, tables) to support your findings.\n",
    "    7. Conducting multi-faceted analysis, considering various aspects of the data.\n",
    "    8. Citing sources accurately using APA citation style.\n",
    "\n",
    "    When writing the report:\n",
    "    - Begin with a clear statement of the research hypothesis in the introduction.\n",
    "    - In the methodology section, provide a detailed explanation of data collection methods, analysis techniques, and any statistical tests used.\n",
    "    - Present results objectively, using appropriate visualizations to illustrate key findings.\n",
    "    - In the discussion section, interpret the results in the context of the research hypothesis and existing literature.\n",
    "    - Include a variety of charts and graphs that effectively communicate different aspects of the data analysis.\n",
    "    - Ensure all visualizations are properly labeled, captioned, and referenced in the text.\n",
    "    - Maintain an objective tone throughout and support all claims with evidence from your analysis or cited sources.\n",
    "    - Use clear, concise language suitable for an academic audience.\n",
    "    - Conclude by summarizing key findings and their implications, relating back to the initial hypothesis.\n",
    "    - Provide a comprehensive list of references for all sources cited in the report.\n",
    "\n",
    "    Remember to integrate insights from other team members (Visualization, Search, and Coder) into your report. Use their outputs to strengthen your analysis and support your conclusions.''',\n",
    "    members,WORKING_DIRECTORY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_review_agent=create_agent(\n",
    "    llm, \n",
    "    [create_document,read_document,edit_document], \n",
    "    '''\n",
    "    You are a meticulous quality control expert responsible for reviewing and ensuring the high standard of all research outputs. Your tasks include:\n",
    "    1. Critically evaluating the content, methodology, and conclusions of research reports.\n",
    "    2. Checking for consistency, accuracy, and clarity in all documents.\n",
    "    3. Identifying areas that need improvement or further elaboration.\n",
    "    4. Ensuring adherence to scientific writing standards and ethical guidelines.\n",
    "\n",
    "    After your review, if revisions are needed, respond with 'REVISION' as a prefix, set needs_revision=True, and provide specific feedback on parts that need improvement. If no revisions are necessary, respond with 'CONTINUE' as a prefix and set needs_revision=False.\n",
    "    ''',\n",
    "    members,WORKING_DIRECTORY\n",
    "    )\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_agent import create_note_agent\n",
    "note_agent=create_note_agent(\n",
    "    json_llm, \n",
    "    [read_document], \n",
    "    '''\n",
    "    You are a detail-oriented research process note-taker. Your primary responsibility is to summarize and document the actions and findings of other team members. Your tasks include:\n",
    "    1. Observing and recording key activities and decisions made by the research team.\n",
    "    2. Summarizing complex information into clear, concise notes.\n",
    "    3. Organizing notes in a structured, easily retrievable format.\n",
    "    4. Highlighting important insights, breakthroughs, or challenges encountered.\n",
    "    5. Only response JSON object\n",
    "    ''',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_node(\"Hypothesis\", lambda state: agent_node(state, hypothesis_agent, \"hypothesis_agent\"))\n",
    "workflow.add_node(\"Process\", lambda state: agent_node(state, process_agent, \"process_agent\"))\n",
    "workflow.add_node(\"Visualization\", lambda state: agent_node(state, visualization_agent, \"visualization_agent\"))\n",
    "workflow.add_node(\"Search\", lambda state: agent_node(state, searcher_agent, \"searcher_agent\"))\n",
    "workflow.add_node(\"Coder\", lambda state: agent_node(state, code_agent, \"code_agent\"))\n",
    "workflow.add_node(\"Report\", lambda state: agent_node(state, report_agent, \"report_agent\"))\n",
    "workflow.add_node(\"QualityReview\", lambda state: agent_node(state, quality_review_agent, \"quality_review_agent\"))\n",
    "workflow.add_node(\"NoteTaker\", lambda state: note_agent_node(state, note_agent, \"note_agent\"))\n",
    "workflow.add_node(\"HumanChoice\", human_choice_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, START\n",
    "\n",
    "workflow.add_edge(\"Hypothesis\", \"HumanChoice\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"HumanChoice\",\n",
    "    hypothesis_router,\n",
    "    {\n",
    "        \"Hypothesis\": \"Hypothesis\",\n",
    "        \"Process\": \"Process\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"Process\",\n",
    "    process_router,\n",
    "    {\n",
    "        \"Coder\": \"Coder\",\n",
    "        \"Search\": \"Search\",\n",
    "        \"Visualization\": \"Visualization\",\n",
    "        \"Report\": \"Report\",\n",
    "        \"Process\": \"Process\",\n",
    "        \"END\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "for member in [\"Visualization\",'Search','Coder','Report']:\n",
    "    workflow.add_edge(member, \"QualityReview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_conditional_edges(\n",
    "    \"QualityReview\",\n",
    "    QualityReview_router,\n",
    "    {\n",
    "        'Visualization': \"Visualization\",\n",
    "        'Search': \"Search\",\n",
    "        'Coder': \"Coder\",\n",
    "        'Report': \"Report\",\n",
    "        'NoteTaker': \"NoteTaker\",\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"NoteTaker\", \"Process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "workflow.add_edge(START, \"Hypothesis\")\n",
    "memory = MemorySaver()\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userInput = '''\n",
    "datapath:./data_storage/OnlineSalesData.csv\n",
    "1. Generate testable hypotheses based on labels or variable names given by the data.\n",
    "\n",
    "2. Build a statistical model of these testable hypotheses.\n",
    "\n",
    "3. Use data to estimate statistical models\n",
    "\n",
    "4. Use the results of statistical model estimation to verify the hypothesis.\n",
    "\n",
    "5. Generate reports or charts based on verified hypotheses.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "events = graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=userInput\n",
    "            ),\n",
    "        ],\n",
    "        \"hypothesis\": \"\",\n",
    "        \"process_decision\":\"\",\n",
    "        \"process\": \"\",\n",
    "        \"visualization_state\": \"\",\n",
    "        \"searcher_state\": \"\",\n",
    "        \"code_state\": \"\",\n",
    "        \"report_section\": \"\",\n",
    "        \"quality_review\": \"\",\n",
    "        \"needs_revision\": False,\n",
    "        \"last_sender\": \"\",\n",
    "    },\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}, \"recursion_limit\": 300},\n",
    "    stream_mode=\"values\",\n",
    "    debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream(stream):\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message,end='',flush=True)\n",
    "        else:\n",
    "            message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stream(events)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
